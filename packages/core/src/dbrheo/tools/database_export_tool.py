"""
DatabaseExportTool - æ™ºèƒ½æ•°æ®å¯¼å‡ºå·¥å…·
ç›´æ¥å°†SQLæŸ¥è¯¢ç»“æœå¯¼å‡ºåˆ°æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼Œè§£å†³Agentæ‰‹åŠ¨æ‹¼æ¥æ•°æ®çš„ç—›ç‚¹
"""

import os
import csv
import json
import re
from pathlib import Path
from typing import Dict, Any, Optional, Union, List
from datetime import datetime
from ..types.tool_types import ToolResult
from ..types.core_types import AbortSignal
from .base import DatabaseTool
from ..config.base import DatabaseConfig


class DatabaseExportTool(DatabaseTool):
    """
    æ™ºèƒ½æ•°æ®å¯¼å‡ºå·¥å…·
    è§£å†³Agentéœ€è¦æ‰‹åŠ¨æ‹¼æ¥CSV/JSONçš„ç—›ç‚¹
    æ”¯æŒç›´æ¥ä»SQLæŸ¥è¯¢å¯¼å‡ºåˆ°å„ç§æ ¼å¼æ–‡ä»¶
    """
    
    # é»˜è®¤æ‰¹é‡å¤§å°
    DEFAULT_BATCH_SIZE = 1000
    MAX_BATCH_SIZE = 10000
    
    def _apply_pagination(self, sql: str, batch_size: int, offset: int) -> str:
        """æ™ºèƒ½åº”ç”¨åˆ†é¡µï¼Œé¿å…é‡å¤LIMIT"""
        sql_upper = sql.upper().strip()
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰LIMIT
        if 'LIMIT' in sql_upper:
            # å¦‚æœå·²ç»æœ‰LIMITï¼Œä¸å†æ·»åŠ 
            # Agentå¯ä»¥çœ‹åˆ°: SQLå·²åŒ…å«LIMITï¼Œè·³è¿‡åˆ†é¡µ
            return sql
        else:
            # æ²¡æœ‰LIMITï¼Œæ·»åŠ åˆ†é¡µ
            return f"{sql} LIMIT {batch_size} OFFSET {offset}"
    
    def __init__(self, config: DatabaseConfig, i18n=None):
        # å…ˆä¿å­˜i18nå®ä¾‹ï¼Œä»¥ä¾¿åœ¨åˆå§‹åŒ–æ—¶ä½¿ç”¨
        self._i18n = i18n
        
        super().__init__(
            name="export_data",
            display_name=self._('export_tool_name', default="Data Export") if i18n else "æ•°æ®å¯¼å‡º",
            description="Export SQL query results directly to files with streaming support for large datasets. Formats: CSV, JSON, Excel, SQL. Features: batch processing, custom delimiters, null handling, append mode.",
            parameter_schema={
                "type": "object",
                "properties": {
                    "sql": {
                        "type": "string",
                        "description": "SQL query to export results from"
                    },
                    "output_path": {
                        "type": "string",
                        "description": "Output file path (extension determines format: .csv, .json, .xlsx)"
                    },
                    "format": {
                        "type": "string",
                        "enum": ["csv", "json", "excel", "sql"],
                        "description": "Export format (auto-detected from file extension if not specified)"
                    },
                    "database": {
                        "type": "string",
                        "description": "Database connection name (optional)"
                    },
                    "options": {
                        "type": "object",
                        "description": "Format-specific options",
                        "properties": {
                            "include_headers": {
                                "type": "boolean",
                                "description": "Include column headers (CSV/Excel)",
                                "default": True
                            },
                            "delimiter": {
                                "type": "string",
                                "description": "CSV delimiter",
                                "default": ","
                            },
                            "null_handling": {
                                "type": "string",
                                "enum": ["empty", "null", "NA", "NULL"],
                                "description": "How to represent NULL values",
                                "default": "empty"
                            },
                            "date_format": {
                                "type": "string",
                                "description": "Date format string",
                                "default": "%Y-%m-%d %H:%M:%S"
                            },
                            "batch_size": {
                                "type": "integer",
                                "description": "Batch size for streaming large datasets",
                                "minimum": 100,
                                "maximum": 10000,
                                "default": 1000
                            },
                            "encoding": {
                                "type": "string",
                                "description": "File encoding (auto for system default). Common: utf-8, cp932 (Japanese), gbk (Chinese)",
                                "default": "auto"
                            },
                            "json_indent": {
                                "type": "integer",
                                "description": "JSON indentation (0 for compact)",
                                "default": 2
                            },
                            "append": {
                                "type": "boolean",
                                "description": "Append to existing file instead of overwriting",
                                "default": False
                            }
                        }
                    }
                },
                "required": ["sql", "output_path"]
            },
            is_output_markdown=True,
            can_update_output=True,
            should_summarize_display=False,
            i18n=i18n  # ä¼ é€’i18nç»™åŸºç±»
        )
        self.config = config
        # åŠ¨æ€æ£€æµ‹ç³»ç»Ÿå¹¶è®¾ç½®çµæ´»çš„å¯¼å‡ºè·¯å¾„æƒé™
        default_paths = self._get_system_paths(config)
        self.allowed_export_paths = config.get("export_allowed_paths", default_paths)
        
    def validate_tool_params(self, params: Dict[str, Any]) -> Optional[str]:
        """éªŒè¯å‚æ•°"""
        sql = params.get("sql", "").strip()
        if not sql:
            return self._('export_sql_empty', default="SQL query cannot be empty")
            
        output_path = params.get("output_path", "").strip()
        if not output_path:
            return self._('export_path_empty', default="Output path cannot be empty")
            
        # æ£€æŸ¥è¾“å‡ºè·¯å¾„æ˜¯å¦åœ¨å…è®¸èŒƒå›´å†…
        try:
            resolved_path = self._resolve_output_path(output_path)
            if not self._is_path_allowed(resolved_path):
                return self._('export_path_not_allowed', default="Export not allowed to: {path}", path=output_path)
        except Exception as e:
            return self._('export_path_invalid', default="Invalid output path: {error}", error=str(e))
            
        # éªŒè¯æ ¼å¼
        format_type = params.get("format")
        if not format_type:
            # ä»æ–‡ä»¶æ‰©å±•åæ¨æ–­
            ext = resolved_path.suffix.lower()[1:]  # å»æ‰ç‚¹å·
            if ext not in ["csv", "json", "xlsx", "xls", "sql"]:
                return self._('export_format_unsupported', default="Unsupported file format: {format}", format=ext)
                
        return None
        
    def get_description(self, params: Dict[str, Any]) -> str:
        """è·å–æ“ä½œæè¿°"""
        output_path = params.get("output_path", "")
        format_type = params.get("format")
        
        if not format_type:
            # ä»æ–‡ä»¶æ‰©å±•åæ¨æ–­
            ext = Path(output_path).suffix.lower()[1:]
            format_type = ext
            
        return self._('export_description', default="Export query result to {format} file: {filename}", format=format_type.upper(), filename=Path(output_path).name)
        
    async def should_confirm_execute(self, params: Dict[str, Any], signal: AbortSignal) -> Union[bool, Any]:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ç¡®è®¤"""
        output_path = params.get("output_path", "")
        options = params.get("options", {})
        append = options.get("append", False)
        
        # å¦‚æœæ–‡ä»¶å­˜åœ¨ä¸”ä¸æ˜¯è¿½åŠ æ¨¡å¼ï¼Œéœ€è¦ç¡®è®¤
        try:
            resolved_path = self._resolve_output_path(output_path)
            if resolved_path.exists() and not append:
                return {
                    "title": self._('export_confirm_overwrite_title', default="Confirm file overwrite"),
                    "message": self._('export_confirm_overwrite_message', default="File {filename} already exists, overwrite?", filename=resolved_path.name),
                    "details": self._('export_confirm_overwrite_details', default="Full path: {path}", path=resolved_path)
                }
        except:
            pass
            
        return False
        
    async def execute(
        self,
        params: Dict[str, Any],
        signal: AbortSignal,
        update_output: Optional[Any] = None
    ) -> ToolResult:
        """æ‰§è¡Œæ•°æ®å¯¼å‡º"""
        sql = params.get("sql", "").strip()
        output_path = params.get("output_path", "").strip()
        format_type = params.get("format")
        database = params.get("database")
        options = params.get("options", {})
        
        try:
            # è§£æè¾“å‡ºè·¯å¾„
            resolved_path = self._resolve_output_path(output_path)
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            resolved_path.parent.mkdir(parents=True, exist_ok=True)
            
            # æ¨æ–­æ ¼å¼
            if not format_type:
                ext = resolved_path.suffix.lower()[1:]
                format_map = {
                    "csv": "csv",
                    "json": "json",
                    "xlsx": "excel",
                    "xls": "excel",
                    "sql": "sql"
                }
                format_type = format_map.get(ext, "csv")
                
            if update_output:
                update_output(self._('export_progress', default="Exporting data to {format} format...\nFile: {filename}", format=format_type.upper(), filename=resolved_path.name))
                
            # è·å–æ•°æ®åº“é€‚é…å™¨
            from ..adapters.adapter_factory import get_adapter
            adapter = await get_adapter(self.config, database)
            
            # è¿æ¥æ•°æ®åº“
            await adapter.connect()
            
            try:
                # æ‰§è¡Œå¯¼å‡º
                if format_type == "csv":
                    result = await self._export_csv(sql, resolved_path, adapter, options, update_output)
                elif format_type == "json":
                    result = await self._export_json(sql, resolved_path, adapter, options, update_output)
                elif format_type == "excel":
                    result = await self._export_excel(sql, resolved_path, adapter, options, update_output)
                elif format_type == "sql":
                    result = await self._export_sql(sql, resolved_path, adapter, options, update_output)
                else:
                    return ToolResult(error=f"Unsupported format: {format_type}")
                    
                return result
                
            finally:
                # ç¡®ä¿æ–­å¼€è¿æ¥
                await adapter.disconnect()
                
        except Exception as e:
            return ToolResult(
                error=self._('export_failed_error', default="Export failed: {error}", error=str(e)),
                summary=self._('export_failed_summary', default="Export failed"),
                return_display=self._('export_failed_display', default="âŒ Export failed: {error}", error=str(e))
            )
            
    async def _export_csv(
        self, 
        sql: str, 
        output_path: Path, 
        adapter, 
        options: Dict[str, Any],
        update_output: Optional[Any] = None
    ) -> ToolResult:
        """å¯¼å‡ºä¸ºCSVæ ¼å¼"""
        include_headers = options.get("include_headers", True)
        delimiter = options.get("delimiter", ",")
        null_handling = options.get("null_handling", "empty")
        encoding = self._get_encoding(options)
        batch_size = min(options.get("batch_size", self.DEFAULT_BATCH_SIZE), self.MAX_BATCH_SIZE)
        append = options.get("append", False)
        
        null_value = {
            "empty": "",
            "null": "null",
            "NA": "NA",
            "NULL": "NULL"
        }.get(null_handling, "")
        
        mode = 'a' if append else 'w'
        total_rows = 0
        
        try:
            with open(output_path, mode, newline='', encoding=encoding) as csvfile:
                writer = None
                
                # æµå¼å¤„ç†å¤§æ•°æ®é›†
                offset = 0
                while True:
                    # æ„é€ åˆ†é¡µæŸ¥è¯¢ï¼ˆæ™ºèƒ½å¤„ç†å·²æœ‰çš„LIMITï¼‰
                    paginated_sql = self._apply_pagination(sql, batch_size, offset)
                    result = await adapter.execute_query(paginated_sql)
                    
                    rows = result.get('rows', [])
                    columns = result.get('columns', [])
                    
                    if not rows:
                        break
                        
                    # ç¬¬ä¸€æ‰¹æ•°æ®æ—¶åˆå§‹åŒ–writer
                    if writer is None:
                        writer = csv.DictWriter(
                            csvfile, 
                            fieldnames=columns,
                            delimiter=delimiter
                        )
                        # å†™å…¥è¡¨å¤´ï¼ˆå¦‚æœéœ€è¦ä¸”ä¸æ˜¯è¿½åŠ æ¨¡å¼ï¼‰
                        if include_headers and not (append and output_path.stat().st_size > 0):
                            writer.writeheader()
                            
                    # å†™å…¥æ•°æ®
                    for row in rows:
                        # å¤„ç†NULLå€¼
                        processed_row = {}
                        for key, value in row.items():
                            if value is None:
                                processed_row[key] = null_value
                            else:
                                processed_row[key] = str(value)
                        writer.writerow(processed_row)
                        
                    total_rows += len(rows)
                    offset += batch_size
                    
                    if update_output and total_rows % (batch_size * 10) == 0:
                        update_output(self._('export_rows_progress', default="Exported {count:,} rows...", count=total_rows))
                        
                    # å¦‚æœè¿”å›çš„è¡Œæ•°å°‘äºæ‰¹é‡å¤§å°ï¼Œè¯´æ˜å·²ç»åˆ°æœ€åäº†
                    if len(rows) < batch_size:
                        break
                        
            # è·å–æ–‡ä»¶å¤§å°
            file_size = output_path.stat().st_size
            
            return ToolResult(
                summary=self._('export_csv_success', default="Successfully exported {count:,} rows to CSV file", count=total_rows),
                llm_content={
                    'export_result': {
                        'format': 'csv',
                        'rows_exported': total_rows,
                        'file_path': str(output_path),
                        'file_size': file_size,
                        'options': options
                    }
                },
                return_display=self._('export_csv_success_display', default="âœ… Export successful\nğŸ“„ File: {filename}\nğŸ“Š Format: CSV\nğŸ“ Rows: {rows:,}\nğŸ’¾ Size: {size}", filename=output_path.name, rows=total_rows, size=self._format_size(file_size))
            )
            
        except Exception as e:
            # Agentè°ƒè¯•: åŸSQL={sql[:50]}...
            return ToolResult(
                error=self._('export_csv_failed', default="CSV export failed: {error}", error=str(e)),
                summary=self._('export_csv_failed_summary', default="CSV export failed"),
                llm_content={"debug": {"original_sql": sql[:100], "error": str(e)}}
            )
            
    async def _export_json(
        self,
        sql: str,
        output_path: Path,
        adapter,
        options: Dict[str, Any],
        update_output: Optional[Any] = None
    ) -> ToolResult:
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        indent = options.get("json_indent", 2)
        encoding = self._get_encoding(options)
        batch_size = min(options.get("batch_size", self.DEFAULT_BATCH_SIZE), self.MAX_BATCH_SIZE)
        append = options.get("append", False)
        date_format = options.get("date_format", "%Y-%m-%d %H:%M:%S")
        
        total_rows = 0
        all_data = [] if not append else None
        
        try:
            # å¦‚æœæ˜¯è¿½åŠ æ¨¡å¼ï¼Œå…ˆè¯»å–ç°æœ‰æ•°æ®
            if append and output_path.exists():
                with open(output_path, 'r', encoding=encoding) as f:
                    try:
                        all_data = json.load(f)
                        if not isinstance(all_data, list):
                            all_data = [all_data]
                    except:
                        all_data = []
            elif not append:
                all_data = []
                
            # æµå¼å¤„ç†æ•°æ®
            offset = 0
            while True:
                paginated_sql = self._apply_pagination(sql, batch_size, offset)
                result = await adapter.execute_query(paginated_sql)
                
                rows = result.get('rows', [])
                if not rows:
                    break
                    
                # å¤„ç†æ—¥æœŸæ—¶é—´å¯¹è±¡
                for row in rows:
                    for key, value in row.items():
                        if isinstance(value, datetime):
                            row[key] = value.strftime(date_format)
                            
                if all_data is not None:
                    all_data.extend(rows)
                else:
                    # å¯¹äºå¤§æ•°æ®é›†ï¼Œä½¿ç”¨æµå¼JSONï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
                    mode = 'a' if append else 'w'
                    with open(output_path, mode, encoding=encoding) as f:
                        for row in rows:
                            json.dump(row, f, ensure_ascii=False)
                            f.write('\n')
                            
                total_rows += len(rows)
                offset += batch_size
                
                if update_output and total_rows % (batch_size * 10) == 0:
                    update_output(self._('export_rows_progress', default="å·²å¯¼å‡º {count:,} è¡Œ...", count=total_rows))
                    
                if len(rows) < batch_size:
                    break
                    
            # å†™å…¥å®Œæ•´çš„JSONæ•°ç»„ï¼ˆå¦‚æœä¸æ˜¯æµå¼ï¼‰
            if all_data is not None:
                with open(output_path, 'w', encoding=encoding) as f:
                    json.dump(all_data, f, ensure_ascii=False, indent=indent)
                    
            file_size = output_path.stat().st_size
            
            return ToolResult(
                summary=self._('export_json_success', default="Successfully exported {count:,} rows to JSON file", count=total_rows),
                llm_content={
                    'export_result': {
                        'format': 'json',
                        'rows_exported': total_rows,
                        'file_path': str(output_path),
                        'file_size': file_size,
                        'streaming': all_data is None
                    }
                },
                return_display=self._('export_json_success_display', default="âœ… Export successful\nğŸ“„ File: {filename}\nğŸ“Š Format: JSON\nğŸ“ Rows: {rows:,}\nğŸ’¾ Size: {size}", filename=output_path.name, rows=total_rows, size=self._format_size(file_size))
            )
            
        except Exception as e:
            return ToolResult(
                error=self._('export_json_failed', default="JSON export failed: {error}", error=str(e)),
                summary=self._('export_json_failed_summary', default="JSON export failed")
            )
            
    async def _export_excel(
        self,
        sql: str,
        output_path: Path,
        adapter,
        options: Dict[str, Any],
        update_output: Optional[Any] = None
    ) -> ToolResult:
        """å¯¼å‡ºä¸ºExcelæ ¼å¼"""
        try:
            # å°è¯•å¯¼å…¥openpyxl
            try:
                import openpyxl
                from openpyxl import Workbook
            except ImportError:
                return ToolResult(
                    error=self._('export_excel_missing_lib', default="Excel export requires 'openpyxl' package. Please install it: pip install openpyxl"),
                    summary=self._('export_excel_missing_lib_summary', default="Missing Excel support library")
                )
                
            include_headers = options.get("include_headers", True)
            batch_size = min(options.get("batch_size", self.DEFAULT_BATCH_SIZE), self.MAX_BATCH_SIZE)
            
            # åˆ›å»ºå·¥ä½œç°¿
            wb = Workbook()
            ws = wb.active
            ws.title = "Query Results"
            
            total_rows = 0
            offset = 0
            headers_written = False
            
            # æµå¼å¤„ç†æ•°æ®
            while True:
                paginated_sql = self._apply_pagination(sql, batch_size, offset)
                result = await adapter.execute_query(paginated_sql)
                
                rows = result.get('rows', [])
                columns = result.get('columns', [])
                
                if not rows:
                    break
                    
                # å†™å…¥è¡¨å¤´
                if include_headers and not headers_written:
                    ws.append(columns)
                    headers_written = True
                    
                # å†™å…¥æ•°æ®
                for row in rows:
                    row_data = [row.get(col) for col in columns]
                    ws.append(row_data)
                    
                total_rows += len(rows)
                offset += batch_size
                
                if update_output and total_rows % (batch_size * 10) == 0:
                    update_output(self._('export_rows_progress', default="å·²å¯¼å‡º {count:,} è¡Œ...", count=total_rows))
                    
                if len(rows) < batch_size:
                    break
                    
            # ä¿å­˜æ–‡ä»¶
            wb.save(output_path)
            file_size = output_path.stat().st_size
            
            return ToolResult(
                summary=self._('export_excel_success', default="Successfully exported {count:,} rows to Excel file", count=total_rows),
                llm_content={
                    'export_result': {
                        'format': 'excel',
                        'rows_exported': total_rows,
                        'file_path': str(output_path),
                        'file_size': file_size
                    }
                },
                return_display=self._('export_excel_success_display', default="âœ… Export successful\nğŸ“„ File: {filename}\nğŸ“Š Format: Excel\nğŸ“ Rows: {rows:,}\nğŸ’¾ Size: {size}", filename=output_path.name, rows=total_rows, size=self._format_size(file_size))
            )
            
        except Exception as e:
            return ToolResult(
                error=self._('export_excel_failed', default="Excel export failed: {error}", error=str(e)),
                summary=self._('export_excel_failed_summary', default="Excel export failed")
            )
            
    async def _export_sql(
        self,
        sql: str,
        output_path: Path,
        adapter,
        options: Dict[str, Any],
        update_output: Optional[Any] = None
    ) -> ToolResult:
        """å¯¼å‡ºä¸ºSQL INSERTè¯­å¥"""
        encoding = self._get_encoding(options)
        batch_size = min(options.get("batch_size", self.DEFAULT_BATCH_SIZE), self.MAX_BATCH_SIZE)
        
        # å°è¯•ä»SQLä¸­æå–è¡¨åï¼ˆç®€å•å®ç°ï¼‰
        sql_upper = sql.upper()
        table_name = "exported_data"  # Default table name
        if "FROM" in sql_upper:
            from_pos = sql_upper.find("FROM")
            # ç®€å•æå–FROMåçš„ç¬¬ä¸€ä¸ªè¯ä½œä¸ºè¡¨å
            after_from = sql[from_pos + 4:].strip()
            table_name = after_from.split()[0].strip('`"[]')
            
        total_rows = 0
        
        try:
            with open(output_path, 'w', encoding=encoding) as f:
                # å†™å…¥å¤´éƒ¨æ³¨é‡Š
                f.write(self._('export_sql_header_1', default="-- Exported from DbRheo on {date}\n", date=datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
                f.write(self._('export_sql_header_2', default="-- Original query: {sql}\n\n", sql=sql))
                
                offset = 0
                while True:
                    paginated_sql = self._apply_pagination(sql, batch_size, offset)
                    result = await adapter.execute_query(paginated_sql)
                    
                    rows = result.get('rows', [])
                    columns = result.get('columns', [])
                    
                    if not rows:
                        break
                        
                    # ç”ŸæˆINSERTè¯­å¥
                    for row in rows:
                        values = []
                        for col in columns:
                            value = row.get(col)
                            if value is None:
                                values.append("NULL")
                            elif isinstance(value, (int, float)):
                                values.append(str(value))
                            else:
                                # è½¬ä¹‰å•å¼•å·
                                escaped = str(value).replace("'", "''")
                                values.append(f"'{escaped}'")
                                
                        insert_sql = f"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(values)});\n"
                        f.write(insert_sql)
                        
                    total_rows += len(rows)
                    offset += batch_size
                    
                    if update_output and total_rows % (batch_size * 10) == 0:
                        update_output(self._('export_rows_progress', default="Exported {count:,} rows...", count=total_rows))
                        
                    if len(rows) < batch_size:
                        break
                        
            file_size = output_path.stat().st_size
            
            return ToolResult(
                summary=self._('export_sql_success', default="Successfully exported {count:,} rows to SQL file", count=total_rows),
                llm_content={
                    'export_result': {
                        'format': 'sql',
                        'rows_exported': total_rows,
                        'file_path': str(output_path),
                        'file_size': file_size,
                        'table_name': table_name
                    }
                },
                return_display=self._('export_sql_success_display', default="âœ… Export successful\nğŸ“„ File: {filename}\nğŸ“Š Format: SQL INSERT\nğŸ“ Rows: {rows:,}\nğŸ’¾ Size: {size}", filename=output_path.name, rows=total_rows, size=self._format_size(file_size))
            )
            
        except Exception as e:
            return ToolResult(
                error=self._('export_sql_failed', default="SQL export failed: {error}", error=str(e)),
                summary=self._('export_sql_failed_summary', default="SQL export failed")
            )
    
    def _get_system_paths(self, config) -> list:
        """åŠ¨æ€æ£€æµ‹ç³»ç»Ÿå¹¶è¿”å›åˆé€‚çš„è®¿é—®è·¯å¾„ - çœŸæ­£çš„çµæ´»æ€§"""
        paths = []
        
        # æ™ºèƒ½æ£€æµ‹é¡¹ç›®æ ¹ç›®å½•ï¼ˆå¾€ä¸Šæ‰¾åˆ°åŒ…å«packagesçš„ç›®å½•ï¼‰
        working_dir = Path(config.get_working_dir())
        current_dir = working_dir
        
        # å‘ä¸ŠæŸ¥æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
        while current_dir.parent != current_dir:  # æ²¡åˆ°æ ¹ç›®å½•
            if (current_dir / 'packages').exists() or (current_dir / 'pyproject.toml').exists():
                paths.append(str(current_dir))
                break
            current_dir = current_dir.parent
        
        # å¦‚æœæ²¡æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œè‡³å°‘åŒ…å«å·¥ä½œç›®å½•
        paths.append(config.get_working_dir())
        
        # ç”¨æˆ·ä¸»ç›®å½• - è·¨å¹³å°é€šç”¨
        home_dir = os.path.expanduser("~")
        if home_dir and os.path.exists(home_dir):
            paths.append(home_dir)
        
        # æ ¹æ®ç³»ç»Ÿå¹³å°åŠ¨æ€æ·»åŠ æ ¹è·¯å¾„
        import platform
        system = platform.system().lower()
        
        if system == "windows":
            # Windows: åŠ¨æ€æ£€æµ‹æ‰€æœ‰å¯ç”¨é©±åŠ¨å™¨
            import string
            for drive in string.ascii_uppercase:
                drive_path = f"{drive}:\\"
                if os.path.exists(drive_path):
                    paths.append(drive_path)
        
        elif system == "darwin":  # macOS
            paths.extend([
                "/",              # æ ¹ç›®å½•
                "/Users",         # ç”¨æˆ·ç›®å½•
                "/Applications",  # åº”ç”¨ç¨‹åº
                "/Volumes",       # æŒ‚è½½ç‚¹
            ])
        
        elif system == "linux":
            paths.extend([
                "/",              # æ ¹ç›®å½•
                "/home",          # ç”¨æˆ·ç›®å½•
                "/mnt",           # æŒ‚è½½ç‚¹ (WSLç­‰)
                "/media",         # åª’ä½“æŒ‚è½½
                "/opt",           # å¯é€‰è½¯ä»¶
                "/tmp",           # ä¸´æ—¶ç›®å½•
            ])
        
        else:
            # æœªçŸ¥ç³»ç»Ÿï¼Œä½¿ç”¨é€šç”¨è·¯å¾„
            if os.path.exists("/"):
                paths.append("/")
            # å°è¯•æ£€æµ‹å¸¸è§æŒ‚è½½ç‚¹
            for mount_point in ["/mnt", "/media", "/Volumes"]:
                if os.path.exists(mount_point):
                    paths.append(mount_point)
        
        # è¿‡æ»¤æ‰ä¸å­˜åœ¨çš„è·¯å¾„ï¼Œä¿ç•™çœŸå®å¯è®¿é—®çš„
        return [p for p in paths if os.path.exists(p)]
            
    def _resolve_output_path(self, path: str) -> Path:
        """è§£æè¾“å‡ºè·¯å¾„"""
        p = Path(path)
        
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºå·¥ä½œç›®å½•
        if not p.is_absolute():
            working_dir = Path(self.config.get_working_dir())
            p = working_dir / p
            
        return p.resolve()
        
    def _is_path_allowed(self, path: Path) -> bool:
        """æ£€æŸ¥è·¯å¾„æ˜¯å¦åœ¨å…è®¸çš„å¯¼å‡ºç›®å½•å†…"""
        for allowed_path in self.allowed_export_paths:
            allowed = Path(allowed_path).resolve()
            try:
                path.relative_to(allowed)
                return True
            except ValueError:
                continue
        return False
        
    def _format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} TB"
    
    def _get_encoding(self, options: Dict[str, Any]) -> str:
        """è·å–ç¼–ç è®¾ç½® - æ”¯æŒè‡ªåŠ¨æ£€æµ‹"""
        encoding_param = options.get("encoding", "auto")
        
        if encoding_param == "auto":
            try:
                from ..utils.encoding_utils import get_system_encoding
                return get_system_encoding()
            except:
                return "utf-8"
        else:
            return encoding_param